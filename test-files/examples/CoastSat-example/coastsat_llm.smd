```python exec
# This code makes the micropublication aware of its interface data.
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import LinearRegression
from typing import Optional, List, Dict, Any
from plotly.subplots import make_subplots
from shapely.geometry import LineString
from rocrate.rocrate import ROCrate
from dataclasses import dataclass
from types import SimpleNamespace
from datetime import datetime
from pathlib import Path
from scipy import stats
from math import sqrt

import plotly.graph_objects as go
import matplotlib.pyplot as plt
import plotly.express as px
import geopandas as gpd
import contextily as cx 
import pandas as pd
import numpy as np
import statistics
import requests
import folium
import json
import os
import re

# Set the site_id manually
site_id = "nzd0001"
```

```python exec 
# Load the publication.crate, interface.crate, and batch_processes.crate manifest files (if available)
# For deployment, we primarily rely on cached data files
interface_crate_path = Path.cwd() / "interface.crate" 
batch_processes_crate_path = interface_crate_path / "batch_processes"

# Check for cached data files
cached_shoreline_path = Path.cwd() / "cached_shoreline.geojson"
cached_primary_result_path = Path.cwd() / "cached_primary_result.geojson"

try:
    if interface_crate_path.exists() and batch_processes_crate_path.exists():
        interface_crate = ROCrate(interface_crate_path)
        batch_processes_crate = ROCrate(batch_processes_crate_path)
    else:
        print("Local crate files not found - using cached data files only")
        interface_crate = None
        batch_processes_crate = None
except Exception as e:
    interface_crate = None
    batch_processes_crate = None
    print(f"Error loading publication crate (will use cached data): {e}")
```

```python exec
# Function to extract commit date from GitHub API
def get_commit_date(github_commit_url):
    """Extract commit date from a GitHub commit URL"""
    if not github_commit_url:
        return None
    
    # Extract repo and commit hash from URL
    import re
    match = re.match(r"https://github\.com/(.+)/(.+)/commit/([a-f0-9]+)", github_commit_url)
    if not match:
        return None
    
    owner, repo, commit_hash = match.groups()
    
    try:
        # Make API request to get commit info
        import requests
        api_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit_hash}"
        response = requests.get(api_url)
        if response.status_code == 200:
            commit_data = response.json()
            commit_date = commit_data['commit']['committer']['date']
            # Convert to readable format
            from datetime import datetime
            dt = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
            return dt.strftime('%B %d, %Y')
        else:
            return None
    except Exception as e:
        print(f"Error fetching commit date: {e}")
        return None

# Extract version information
coastsat_version = interface_crate.mainEntity.get("version") if interface_crate else None
coastsat_commit_date = get_commit_date(coastsat_version)
interface_crate_version = interface_crate.mainEntity.get("url") if interface_crate else None
```

```python exec 
# Before narrative content, some helper functions to interface with the crates:
def query_by_link(crate, prop, target_id, match_substring=False):
    """
    Return entities (dict or ContextEntity) whose `prop` links to `target_id`.
    If `match_substring` is True, will return entities whose link includes `target_id` as a substring.
    """
    is_rocrate = hasattr(crate, "get_entities")
    entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])
    out = []

    for e in entities:
        val = (e.properties().get(prop) if is_rocrate else e.get(prop))
        if val is None:
            continue
        vals = [val] if not isinstance(val, list) else val

        ids = [
            (x.id if hasattr(x, "id") else x.get("@id") if isinstance(x, dict) else x)
            for x in vals
        ]
        if match_substring:
            if any(target_id in _id for _id in ids if isinstance(_id, str)):
                out.append(e)
        else:
            if target_id in ids:
                out.append(e)
    return out

def filter_linked_entities_by_substring(crate, entities, prop, substring):
    """
    For a given list of entities, follow `prop` links (e.g., 'object') and return
    all linked entities whose `@id` includes `substring`.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Index entities by ID for fast lookup
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }
    matched = []
    for entity in entities:
        val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
        if val is None:
            continue

        vals = [val] if not isinstance(val, list) else val

        for v in vals:
            target_id = (
                v.id if hasattr(v, "id") else v.get("@id") if isinstance(v, dict) else v
            )

            if not isinstance(target_id, str):
                continue

            if substring in target_id:
                linked_entity = id_index.get(target_id)
                if linked_entity:
                    matched.append(linked_entity)

    return matched

def resolve_linked_entity(crate, entity, prop):
    """
    Follow a single-valued property (like 'instrument') from an entity,
    and return the linked entity (resolved from the crate).
    Returns None if the property is missing or not resolvable.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Build a lookup of @id to entity
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }

    val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
    if val is None:
        return None

    # Normalize to string ID
    if isinstance(val, list):
        raise ValueError(f"Expected only one linked entity in property '{prop}', but found a list.")
    
    target_id = (
        val.id if hasattr(val, "id") else val.get("@id") if isinstance(val, dict) else val
    )

    return id_index.get(target_id)

def convert_to_raw_url(github_url: str) -> str:
    """
    Converts a GitHub blob URL to a raw.githubusercontent URL.
    """
    match = re.match(r"https://github\.com/(.+)/blob/([a-f0-9]+)/(.+)", github_url)
    if not match:
        raise ValueError("Invalid GitHub blob URL format.")
    user_repo, commit_hash, path = match.groups()
    return f"https://raw.githubusercontent.com/{user_repo}/{commit_hash}/{path}"

def get_entity_by_id(crate, entity_id, crate_path=None):
    """Get an entity by its ID from the crate - tries multiple ID formats"""
    if not crate:
        return None
    
    # Try different ID formats
    id_variants = [entity_id, f"./{entity_id}", entity_id.replace("./", "")]
    
    try:
        # Method 1: Try accessing entities directly
        if hasattr(crate, 'entities'):
            entities = crate.entities() if callable(crate.entities) else crate.entities
            if isinstance(entities, dict):
                for variant in id_variants:
                    if variant in entities:
                        return entities[variant]
        
        # Method 2: Try get_entity method if available
        if hasattr(crate, 'get_entity'):
            for variant in id_variants:
                try:
                    entity = crate.get_entity(variant)
                    if entity:
                        return entity
                except:
                    pass
        
        # Method 3: Try iterating through entities
        try:
            entities_iter = crate.get_entities() if hasattr(crate, 'get_entities') else None
            if entities_iter:
                for entity in entities_iter:
                    e_id = None
                    if isinstance(entity, dict):
                        e_id = entity.get('@id') or entity.get('id')
                    elif hasattr(entity, 'id'):
                        e_id = entity.id
                    elif hasattr(entity, 'properties'):
                        props = entity.properties
                        if isinstance(props, dict):
                            e_id = props.get('@id') or props.get('id')
                    
                    if e_id and e_id in id_variants:
                        return entity
        except:
            pass
        
        # Method 4: Fallback - read metadata JSON directly
        if not crate_path:
            if hasattr(crate, 'source'):
                crate_path = Path(crate.source)
            elif hasattr(crate, '_source'):
                crate_path = Path(crate._source)
        
        if crate_path:
            if isinstance(crate_path, str):
                crate_path = Path(crate_path)
            if crate_path.is_file():
                crate_path = crate_path.parent
            metadata_file = crate_path / "ro-crate-metadata.json"
            if not metadata_file.exists():
                metadata_file = crate_path / "ro-crate-metadata.jsonld"
            
            if metadata_file.exists():
                with open(metadata_file) as f:
                    metadata = json.load(f)
                    graph = metadata.get('@graph', [])
                    for item in graph:
                        item_id = item.get('@id', item.get('id', ''))
                        if item_id in id_variants:
                            return item
    except Exception as e:
        print(f"Error in get_entity_by_id for {entity_id}: {e}")
        pass
    
    return None

def extract_property(entity, prop_name):
    """Extract a property from an entity, handling different formats"""
    if not entity:
        return None
    if isinstance(entity, dict):
        return entity.get(prop_name) or entity.get(f"@{prop_name}")
    if hasattr(entity, prop_name):
        return getattr(entity, prop_name)
    if hasattr(entity, 'properties'):
        props = entity.properties
        if isinstance(props, dict):
            return props.get(prop_name) or props.get(f"@{prop_name}")
    return None

def normalize_linked_value(value):
    """Normalize linked values (dict with @id, or object with id) to string ID"""
    if value is None:
        return None
    if isinstance(value, str):
        return value
    if isinstance(value, dict):
        return value.get('@id') or value.get('id')
    if hasattr(value, 'id'):
        return value.id
    return str(value) if value else None

def resolve_step_entity(crate, step_ref, crate_path=None):
    """Resolve a step reference (dict with @id) to full entity details"""
    if isinstance(step_ref, dict):
        step_id = step_ref.get('@id') or step_ref.get('id')
    elif isinstance(step_ref, str):
        step_id = step_ref
    else:
        step_id = getattr(step_ref, 'id', None) if hasattr(step_ref, 'id') else None
    
    if not step_id:
        return None
    
    entity = get_entity_by_id(crate, step_id, crate_path=crate_path)
    if not entity:
        return None
    
    # Extract entity properties into a dict
    step_details = {
        'id': step_id,
        'type': extract_property(entity, 'type'),
        'name': extract_property(entity, 'name'),
        'codeRepository': extract_property(entity, 'codeRepository'),
        'encodingFormat': extract_property(entity, 'encodingFormat'),
        'position': extract_property(entity, 'position'),
        'sha256': extract_property(entity, 'sha256'),
    }
    
    # Normalize type if it's a list
    if isinstance(step_details['type'], list):
        step_details['type'] = step_details['type'][0] if step_details['type'] else None
    
    # Handle nested references
    prog_lang = extract_property(entity, 'programmingLanguage')
    step_details['programmingLanguage'] = normalize_linked_value(prog_lang)
    
    example_work = extract_property(entity, 'exampleOfWork')
    if example_work:
        if isinstance(example_work, list):
            step_details['exampleOfWork'] = [normalize_linked_value(v) for v in example_work]
        else:
            step_details['exampleOfWork'] = normalize_linked_value(example_work)
    else:
        step_details['exampleOfWork'] = None
    
    # Handle input/output arrays (list of references)
    input_refs = extract_property(entity, 'input')
    if input_refs:
        if isinstance(input_refs, list):
            step_details['input'] = [normalize_linked_value(v) for v in input_refs]
        else:
            step_details['input'] = [normalize_linked_value(input_refs)]
    else:
        step_details['input'] = []
    
    output_refs = extract_property(entity, 'output')
    if output_refs:
        if isinstance(output_refs, list):
            step_details['output'] = [normalize_linked_value(v) for v in output_refs]
        else:
            step_details['output'] = [normalize_linked_value(output_refs)]
    else:
        step_details['output'] = []
    
    return step_details

# Extract workflow steps from update.sh
workflow_steps = {}
if interface_crate:
    # Pass the crate path for fallback lookup
    update_sh_entity = get_entity_by_id(interface_crate, "update.sh", interface_crate_path)
    if update_sh_entity:
        steps = extract_property(update_sh_entity, 'step')
        if steps:
            steps_list = steps if isinstance(steps, list) else [steps]
            for step_ref in steps_list:
                step_details = resolve_step_entity(interface_crate, step_ref, interface_crate_path)
                if step_details:
                    step_id = step_details['id']
                    workflow_steps[step_id] = step_details
            
            # Sort workflow steps by position if available
            # Convert to list of (position, step_id, step_details) tuples, sort, then rebuild dict
            steps_with_pos = []
            for step_id, step_details in workflow_steps.items():
                position = step_details.get('position')
                # Convert position to int if it's a string number
                try:
                    pos_int = int(position) if position else 999
                except (ValueError, TypeError):
                    pos_int = 999
                steps_with_pos.append((pos_int, step_id, step_details))
            
            # Sort by position and rebuild as ordered dict (Python 3.7+ maintains insertion order)
            steps_with_pos.sort(key=lambda x: x[0])
            workflow_steps = {step_id: step_details for _, step_id, step_details in steps_with_pos}
            print(workflow_steps)
            print("hi")

def get_location_name(geometry):
    """
    Get a human-readable location name from a shapely geometry using reverse geocoding.
    Uses the centroid of the geometry for the lookup.
    """
    try:
        # Get the centroid of the geometry
        centroid = geometry.centroid
        lon, lat = centroid.x, centroid.y
        
        # Fix longitude coordinates that are in 0-360 range (convert to -180 to 180)
        if lon > 180:
            lon = lon - 360
        
        location = requests.get(
            url="https://nominatim.openstreetmap.org/reverse",
            params={
                'lat': lat,
                'lon': lon,
                'format': 'json',
                'zoom': 10,
                'addressdetails': 1
            },
            headers={'User-Agent': 'CoastSat-shoreline-publication'}
        ).json().get('display_name')
        
        return location
    except Exception as e:
        print(f"Error getting location name: {e}")
        return "Unknown location"

def get_appended_rows(
    old_url: str,
    new_url: str,
    filter_column: str = None,
    filter_value: str = None
) -> pd.DataFrame:
    raw_old_url = convert_to_raw_url(old_url)
    raw_new_url = convert_to_raw_url(new_url)
    
    old_df = pd.read_csv(raw_old_url)
    new_df = pd.read_csv(raw_new_url)
    
    appended_df = pd.concat([new_df, old_df]).drop_duplicates(keep=False)

    # If you're looking for a particular transect column (e.g., 'sar0003-0003'),
    # this assumes the filter_column is in the wide format as a column name.
    if filter_column and filter_column in appended_df.columns:
        # Only keep rows where that transect column is not NaN
        appended_df = appended_df[~appended_df[filter_column].isna()]

    return appended_df
```

```python exec
# Plotly visualisation functions
def plot_zone_transect_timeseries(site_time_series_df, zone_transects, zone_metrics):
    # Create a time series plot for the zone transects
    if not site_time_series_df.empty and zone_transects:
        site_time_series_df['dates'] = pd.to_datetime(site_time_series_df['dates'])
        fig = go.Figure()

        # Get valid transects
        transect_ids = [t['id'] for t in zone_transects]
        valid_transects = [(tid, site_time_series_df[['dates', tid]].dropna()) 
                           for tid in transect_ids if tid in site_time_series_df.columns]

        # Filter out transects with insufficient data
        valid_transects = [(tid, data) for tid, data in valid_transects 
                          if len(data) > 0 and len(data.columns) >= 2]

        if valid_transects:
            # Zone baseline: average of all values
            all_values = []
            for _, data in valid_transects:
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    all_values.extend(data.iloc[:, 1].tolist())
            
            if not all_values:
                print("No valid transect data available for zone analysis.")
                return
                
            zone_baseline = np.mean(all_values)

            # Efficient lookup with dicts for each transect
            transect_dicts = []
            for tid, data in valid_transects:
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    transect_dicts.append(
                        (tid, dict(zip(data['dates'], data.iloc[:, 1] - zone_baseline)))
                    )

            all_dates = sorted(set().union(*[data['dates'] for _, data in valid_transects]))

            # Compute zone average over time
            zone_data = []
            for date in all_dates:
                date_vals = [tdict[date] for _, tdict in transect_dicts if date in tdict]
                if date_vals:
                    zone_data.append((date, np.mean(date_vals)))

            zone_dates, zone_values = zip(*zone_data) if zone_data else ([], [])

            # Colors
            colors = px.colors.qualitative.Set1

            # Plot all individual transects (all visible)
            for i, (tid, data) in enumerate(valid_transects):
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    fig.add_trace(go.Scatter(
                        x=data['dates'],
                        y=data.iloc[:, 1] - zone_baseline,
                        mode='lines',
                        name=tid,
                        opacity=0.6,
                        visible=True,  # Show all
                        line=dict(color=colors[i % len(colors)], width=1),
                        hovertemplate='%{fullData.name}: %{y:.1f}m<extra></extra>'
                    ))

            # Smoothed zone average
            if len(zone_values) > 1:
                zone_array = np.array(zone_values)
                window = min(6, len(zone_array) // 4)
                if window >= 2:
                    kernel = np.ones(window) / window
                    smoothed = np.convolve(zone_array, kernel, mode='same')
                else:
                    smoothed = zone_array

                fig.add_trace(go.Scatter(
                    x=zone_dates,
                    y=smoothed,
                    mode='lines+markers',
                    name='Zone Average (Smoothed)',
                    line=dict(color='black', width=3),
                    marker=dict(size=4, color='black'),
                    hovertemplate='Zone Average: %{y:.1f}m<extra></extra>'
                ))

                # Linear trendline for zone average
                if len(zone_dates) > 2:
                    dates_numeric = pd.to_numeric(pd.Series(zone_dates))
                    slope, intercept, *_ = stats.linregress(dates_numeric, zone_values)
                    trend_line = slope * dates_numeric + intercept
                    annual_rate = slope * 365.25 * 24 * 3600 * 1000  # m/ms → m/yr

                    fig.add_trace(go.Scatter(
                        x=zone_dates,
                        y=trend_line,
                        mode='lines',
                        name=f'Zone Trend ({annual_rate:.1f} m/yr)',
                        line=dict(color='red', width=2, dash='dash'),
                        hovertemplate='Trend: %{y:.1f}m<extra></extra>'
                    ))

            # Add baseline reference
            fig.add_hline(
                y=0,
                line_dash="dot",
                line_color="gray",
                opacity=0.5,
                annotation_text="Zone Baseline",
                annotation_position="bottom right"
            )

            # Layout and styling
            fig.update_layout(
                title=f'Shoreline Position Time Series - {zone_metrics.zone_type} Zone<br><sub>Relative to zone mean baseline</sub>',
                xaxis_title='Date',
                yaxis_title='Cross-shore change [m]',
                hovermode='x unified',
                width=800,
                height=400,
                margin=dict(l=50, r=150, t=60, b=60),
                font=dict(size=10),
                legend=dict(
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02,
                    font=dict(size=9),
                    bordercolor="LightGray",
                    borderwidth=1,
                    itemsizing='trace'
                )
            )

            fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#eee')
            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#eee', hoverformat='.1f')

            # Set x-range to emphasize recent data
            if zone_dates and len(zone_dates) > 0:
                end_date = pd.to_datetime(zone_dates[-1])
                start_date = pd.to_datetime(zone_dates[0])
                data_span = (end_date - start_date).days / 365.25
                view_start = end_date - pd.DateOffset(years=20) if data_span > 20 else start_date - pd.DateOffset(years=1)
                view_end = end_date + pd.DateOffset(years=1)
                fig.update_xaxes(range=[view_start, view_end])

            fig.show()
        else:
            print("No time series data available or no transects in zone.")

def plot_zone_uncertainty_metrics(site_time_series_df, zone_transects, window_size=12):
    def compute_metrics_over_time(df, window_size=window_size):
        df = df.dropna()
        if len(df) < window_size:
            return []
        
        # Check if we have the required columns
        if len(df.columns) < 2:
            print(f"Warning: DataFrame has insufficient columns: {list(df.columns)}")
            return []

        df = df.sort_values('dates')
        df['ordinal'] = pd.to_datetime(df['dates']).map(pd.Timestamp.toordinal)
        metrics = []

        for i in range(len(df) - window_size + 1):
            window = df.iloc[i:i+window_size]
            X = window[['ordinal']]
            
            # Safely access the second column (transect data)
            if window.shape[1] < 2:
                continue
            y = window.iloc[:, 1]
            
            # Skip if y has no valid data
            if len(y) == 0 or y.isna().all():
                continue
                
            model = LinearRegression().fit(X, y)
            preds = model.predict(X)
            r2 = r2_score(y, preds)
            rmse = sqrt(mean_squared_error(y, preds))
            center_date = window['dates'].iloc[window_size // 2]
            metrics.append((center_date, r2, rmse))

        return metrics

    # 2. Generate metrics across all transects
    metrics_data = []
    transect_ids = [t['id'] for t in zone_transects]

    for tid in transect_ids:
        if tid not in site_time_series_df.columns:
            continue
        df = site_time_series_df[['dates', tid]].dropna()
        for date, r2, rmse in compute_metrics_over_time(df, window_size=window_size):
            metrics_data.append({
                'transect_id': tid,
                'date': date,
                'r2': r2,
                'rmse': rmse
            })

    metrics_df = pd.DataFrame(metrics_data)

    # 3. Clean and group by monthly bins
    metrics_df['date'] = pd.to_datetime(metrics_df['date'], errors='coerce')
    metrics_df = metrics_df.dropna(subset=['date'])
    metrics_df = metrics_df[(metrics_df['date'] > '1985-01-01') & (metrics_df['date'] < '2025-01-01')]
    metrics_df['date_bin'] = metrics_df['date'].dt.to_period('M').dt.to_timestamp()

    agg_iqr = metrics_df.groupby('date_bin').agg(
        r2_q1=('r2', lambda x: x.quantile(0.25)),
        r2_q3=('r2', lambda x: x.quantile(0.75)),
        r2_median=('r2', 'median'),
        rmse_q1=('rmse', lambda x: x.quantile(0.25)),
        rmse_q3=('rmse', lambda x: x.quantile(0.75)),
        rmse_median=('rmse', 'median')
    ).reset_index().rename(columns={'date_bin': 'date'})

    # 4. Compute RMSE trend line (linear regression)
    agg_iqr_clean = agg_iqr.dropna(subset=['rmse_median']).copy()
    agg_iqr_clean['ordinal'] = agg_iqr_clean['date'].map(pd.Timestamp.toordinal)

    X = agg_iqr_clean[['ordinal']]
    y = agg_iqr_clean['rmse_median']
    model = LinearRegression().fit(X, y)
    agg_iqr_clean['rmse_trend'] = model.predict(X)

    # 5. Create ghost lines for individual transects
    ghost_r2 = []
    ghost_rmse = []

    for tid in metrics_df['transect_id'].unique():
        df = metrics_df[metrics_df['transect_id'] == tid].sort_values('date')
        
        ghost_r2.append(go.Scatter(
            x=df['date'], y=df['r2'],
            mode='lines',
            line=dict(width=1, color='rgba(34,139,34,0.1)'),  # faint green
            showlegend=False, hoverinfo='skip'
        ))
        
        ghost_rmse.append(go.Scatter(
            x=df['date'], y=df['rmse'],
            mode='lines',
            line=dict(width=1, color='rgba(255,140,0,0.1)'),  # faint orange
            showlegend=False, hoverinfo='skip'
        ))

    # 6. Create final plot with subplots
    fig = make_subplots(
        rows=2, cols=1, shared_xaxes=True,
        subplot_titles=('Median R² Score with IQR Band', 'Median RMSE with IQR Band')
    )

    for trace in ghost_r2:
        fig.add_trace(trace, row=1, col=1)
    for trace in ghost_rmse:
        fig.add_trace(trace, row=2, col=1)

    # Median and IQR plots
    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_median'],
        mode='lines', name='R² Median',
        line=dict(color='seagreen')
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_q3'],
        mode='lines', line=dict(width=0), showlegend=False
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_q1'],
        mode='lines', fill='tonexty',
        fillcolor='rgba(46,139,87,0.3)',
        line=dict(width=0), name='R² IQR'
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_median'],
        mode='lines', name='RMSE Median',
        line=dict(color='darkorange')
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_q3'],
        mode='lines', line=dict(width=0), showlegend=False
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_q1'],
        mode='lines', fill='tonexty',
        fillcolor='rgba(255,165,0,0.3)',
        line=dict(width=0), name='RMSE IQR'
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr_clean['date'], y=agg_iqr_clean['rmse_trend'],
        mode='lines', name='RMSE Trend (Linear Fit)',
        line=dict(color='black', dash='dash')
    ), row=2, col=1)

    # Updated layout for responsiveness
    fig.update_layout(
        autosize=True,
        template='plotly_white',
        hovermode='x unified',
        margin=dict(l=40, r=40, t=80, b=40),
        legend=dict(
            orientation="h",
            yanchor="top",
            y=-0.25,
            xanchor="center",
            x=0.5,
            font=dict(size=10)
        )
    )

    fig.update_yaxes(title_text='R² Score', range=[0, 1], row=1, col=1)
    fig.update_yaxes(title_text='RMSE (m)', row=2, col=1)
    fig.update_xaxes(title_text='Date', row=2, col=1)

    # Show with responsive config
    fig.show(config={'responsive': True})

# Folium map creation function
# Create Maps which visualise transects - composable function for different use cases
def create_transect_map(transects_data, map_title="Transects Map", use_gdf=False):
    """
    Create a Folium map with trend-colored transects.
    
    Args:
        transects_data: Either a list of transect dicts (zone case) or GeoDataFrame (site case)
        map_title: Title to display on the map
        use_gdf: If True, treats transects_data as a GeoDataFrame; if False, as transect list
    
    Returns:
        Folium map object
    """
    
    def get_trend_color(trend, n_points=None):
        """Get color based on trend value, matching the index.html color scheme"""
        # Handle insufficient data points (gray)
        if n_points is not None and n_points < 10:
            return '#BABABA'
        
        # Handle null trends (gray)
        if trend is None or pd.isna(trend):
            return '#808080'
        
        # Apply the same color scale as index.html: RdYlBu with domain [-3, 3]
        # Clamp trend values to the domain range
        clamped_trend = max(-3, min(3, trend))
        
        # RdYlBu color scale interpolation (Red-Yellow-Blue)
        # Red for negative (erosion), Yellow for ~0 (stable), Blue for positive (accretion)
        if clamped_trend <= -2:
            return '#d73027'  # Dark red
        elif clamped_trend <= -1:
            return '#f46d43'  # Red-orange
        elif clamped_trend <= -0.5:
            return '#fdae61'  # Orange
        elif clamped_trend <= 0:
            return '#fee08b'  # Light orange/yellow
        elif clamped_trend <= 0.5:
            return '#e6f598'  # Light green/yellow
        elif clamped_trend <= 1:
            return '#abdda4'  # Light blue/green
        elif clamped_trend <= 2:
            return '#66c2a5'  # Medium blue/green
        else:
            return '#3288bd'  # Blue

    # Convert data to GeoDataFrame if needed
    if use_gdf:
        # transects_data is already a GeoDataFrame (site case)
        gdf = transects_data.copy()
    else:
        # transects_data is a list of transect dicts (zone case)
        features = []
        for t in transects_data:
            coords = t["geometry"]["coordinates"]
            line = LineString(coords)
            # Copy all properties from t["properties"], plus id and site_id at top level
            props = dict(t.get("properties", {}))
            props["id"] = t["id"]
            props["site_id"] = t.get("site_id")
            props["geometry"] = line
            features.append(props)
        gdf = gpd.GeoDataFrame(features, crs="EPSG:4326")

    # Calculate bounds and center
    bounds = gdf.total_bounds
    center = [(bounds[1] + bounds[3]) / 2, (bounds[0] + bounds[2]) / 2]

    # Create map
    m = folium.Map(location=center, zoom_start=15, tiles="OpenStreetMap")

    # Rounding configuration for tooltips
    rounding_map = {
        "orientation": ("°", 1),
        "along_dist": ("m", 1),
        "along_dist_norm": ("", 3),
        "beach_slope": ("", 2),
        "cil": ("", 3),
        "ciu": ("", 3),
        "trend": ("", 2),
        "r2_score": ("", 3),
        "mae": ("m", 1),
        "mse": ("m²", 1),
        "rmse": ("m", 1),
        "intercept": ("", 1)
    }

    # Build tooltip fields and aliases
    # Get available columns from the GeoDataFrame
    available_cols = [col for col in gdf.columns if col not in ["geometry", "site_id"]]
    
    tooltip_fields = []
    tooltip_aliases = []
    
    # Always include ID if available
    if "id" in available_cols:
        tooltip_fields.append("id")
        tooltip_aliases.append("Transect ID:")
    
    # Add other available fields
    for key in available_cols:
        if key != "id":  # Already handled above
            tooltip_fields.append(key)
            alias = key.replace("_", " ").capitalize()
            if key in rounding_map:
                unit, _ = rounding_map[key]
                if unit:
                    alias += f" ({unit})"
            tooltip_aliases.append(alias + ":")

    # Style function for each feature
    def style_function(feature):
        props = feature['properties']
        trend = props.get('trend')
        n_points = props.get('n_points_nonan')
        
        color = get_trend_color(trend, n_points)
        
        return {
            'color': color,
            'weight': 3,
            'opacity': 0.8
        }

    # Add GeoJSON layer with styling and tooltips
    folium.GeoJson(
        gdf.to_json(),
        name="Transects",
        style_function=style_function,
        tooltip=folium.GeoJsonTooltip(fields=tooltip_fields, aliases=tooltip_aliases, sticky=True, localize=True)
    ).add_to(m)

    # Fit map to bounds so all transects are visible
    m.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])
    
    # Add compact legend
    legend_html = '''
    <div style="position: fixed; 
                bottom: 20px; right: 20px; width: 120px; height: auto; 
                background-color: rgba(0, 0, 0, 0.85); border-radius: 4px; 
                color: white; padding: 8px; font-size: 10px; z-index: 9999; line-height: 1.2;">
        <div style="font-weight: bold; margin-bottom: 4px; font-size: 11px;">Trend (m/yr)</div>
        <div style="margin-bottom: 2px;"><i style="background: #3288bd; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>≥3 Accretion</div>
        <div style="margin-bottom: 2px;"><i style="background: #66c2a5; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>2</div>
        <div style="margin-bottom: 2px;"><i style="background: #abdda4; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>1</div>
        <div style="margin-bottom: 2px;"><i style="background: #e6f598; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>0 Stable</div>
        <div style="margin-bottom: 2px;"><i style="background: #fee08b; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>-1</div>
        <div style="margin-bottom: 2px;"><i style="background: #fdae61; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>-2</div>
        <div style="margin-bottom: 2px;"><i style="background: #d73027; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>≤-3 Erosion</div>
        <div style="margin-bottom: 0px;"><i style="background: #BABABA; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>Uncertain</div>
    </div>
    '''
    m.get_root().html.add_child(folium.Element(legend_html))
    
    # Add centered title
    title_html = f'<h2 style="position:absolute;z-index:100000;left:50%;top:10px;transform:translateX(-50%);font-size:1.2em;font-weight:400;color:#444;background:rgba(255,255,255,0.7);padding:4px 12px;border-radius:6px;box-shadow:0 1px 4px rgba(0,0,0,0.08);">{map_title}</h2>'
    m.get_root().html.add_child(folium.Element(title_html))

    return m
```

```python exec
# Get data for the site report - use cached data files
cached_shoreline_path = Path.cwd() / "cached_shoreline.geojson"
if cached_shoreline_path.exists():
    shoreline_gdf = gpd.read_file(cached_shoreline_path)
else:
    print("Error: No cached shoreline data found. Run publication_logic.py to cache data first.")
    raise FileNotFoundError("Cached shoreline data not found")

current_site_data = shoreline_gdf[shoreline_gdf['id'] == site_id] # Filter the shoreline data by the current site_id
```

```python exec
# Get location name in a separate block to avoid serialization issues
try:
    if len(current_site_data) > 0:
        geometry = current_site_data['geometry'].iloc[0]
        location_name = get_location_name(geometry)
    else:
        location_name = "No data found for this site"
except Exception as e:
    print(f"Warning: Could not get location name: {e}")
    location_name = "Unknown location"
```

```python exec
# Get data for the site report - use cached primary result data
cached_primary_result_path = Path.cwd() / "cached_primary_result.geojson"
if cached_primary_result_path.exists():
    primary_result_gdf = gpd.read_file(cached_primary_result_path)
else:
    print("Error: No cached primary result data found. Run publication_logic.py to cache data first.")
    raise FileNotFoundError("Cached primary result data not found")

site_primary_data = primary_result_gdf[primary_result_gdf['site_id'] == site_id]
```



# Site Report: `site_id`{python exec}
### Location: `location_name`{python exec}


::: template_describe [openai/gpt-5] Describe this ro-crate metadata: {{interface_crate.entities}} :::