```css raw
table {
    width: 100%;
    border-collapse: collapse;
    font-family: sans-serif;
    font-size: 14px;
    background-color: #fff;
    color: #333;
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
}

th, td {
    border: 1px solid #ddd;
    padding: 6px 10px !important;
}

th {
    background-color: #f4f4f4;
    font-weight: 600;
}

tr:nth-child(even) {
    background-color: #fafafa;
}

tr:hover {
    background-color: #f1f1f1;
}
```

```python exec
# This code makes the micropublication aware of its interface data.
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import LinearRegression
from typing import Optional, List, Dict, Any
from plotly.subplots import make_subplots
from shapely.geometry import LineString
from rocrate.rocrate import ROCrate
from dataclasses import dataclass
from types import SimpleNamespace
from datetime import datetime
from pathlib import Path
from scipy import stats
from math import sqrt

import plotly.graph_objects as go
import matplotlib.pyplot as plt
import plotly.express as px
import geopandas as gpd
import contextily as cx 
import pandas as pd
import numpy as np
import statistics
import requests
import folium
import json
import os
import re

# Load the id data
data_path = os.path.join(Path.cwd(), "data.json")
with open(data_path, 'r') as f:
    data = json.load(f)
site_id = data.get("id")
```

```python exec 
# Load the publication.crate, interface.crate, and batch_processes.crate manifest files (if available)
# For deployment, we primarily rely on cached data files
publication_crate_path = Path.cwd() 
interface_crate_path = Path.cwd() / "interface.crate" 
batch_processes_crate_path = interface_crate_path / "batch_processes"

# Check for cached data files
cached_shoreline_path = Path.cwd() / "cached_shoreline.geojson"
cached_primary_result_path = Path.cwd() / "cached_primary_result.geojson"

try:
    if publication_crate_path.exists() and interface_crate_path.exists() and batch_processes_crate_path.exists():
        publication_crate = ROCrate(publication_crate_path)
        interface_crate = ROCrate(interface_crate_path)
        batch_processes_crate = ROCrate(batch_processes_crate_path)
    else:
        print("Local crate files not found - using cached data files only")
        publication_crate = None
        interface_crate = None
        batch_processes_crate = None
except Exception as e:
    publication_crate = None
    interface_crate = None
    batch_processes_crate = None
    print(f"Error loading publication crate (will use cached data): {e}")
```

```python exec
# Function to extract commit date from GitHub API
def get_commit_date(github_commit_url):
    """Extract commit date from a GitHub commit URL"""
    if not github_commit_url:
        return None
    
    # Extract repo and commit hash from URL
    import re
    match = re.match(r"https://github\.com/(.+)/(.+)/commit/([a-f0-9]+)", github_commit_url)
    if not match:
        return None
    
    owner, repo, commit_hash = match.groups()
    
    try:
        # Make API request to get commit info
        import requests
        api_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit_hash}"
        response = requests.get(api_url)
        if response.status_code == 200:
            commit_data = response.json()
            commit_date = commit_data['commit']['committer']['date']
            # Convert to readable format
            from datetime import datetime
            dt = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
            return dt.strftime('%B %d, %Y')
        else:
            return None
    except Exception as e:
        print(f"Error fetching commit date: {e}")
        return None

# Extract version information
coastsat_version = interface_crate.mainEntity.get("version") if interface_crate else None
coastsat_commit_date = get_commit_date(coastsat_version)
interface_crate_version = interface_crate.mainEntity.get("url") if interface_crate else None
publication_crate_version = publication_crate.mainEntity.get("url") if publication_crate else None
```

```python exec 
# Before narrative content, some helper functions to interface with the crates:
def query_by_link(crate, prop, target_id, match_substring=False):
    """
    Return entities (dict or ContextEntity) whose `prop` links to `target_id`.
    If `match_substring` is True, will return entities whose link includes `target_id` as a substring.
    """
    is_rocrate = hasattr(crate, "get_entities")
    entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])
    out = []

    for e in entities:
        val = (e.properties().get(prop) if is_rocrate else e.get(prop))
        if val is None:
            continue
        vals = [val] if not isinstance(val, list) else val

        ids = [
            (x.id if hasattr(x, "id") else x.get("@id") if isinstance(x, dict) else x)
            for x in vals
        ]
        if match_substring:
            if any(target_id in _id for _id in ids if isinstance(_id, str)):
                out.append(e)
        else:
            if target_id in ids:
                out.append(e)
    return out

def filter_linked_entities_by_substring(crate, entities, prop, substring):
    """
    For a given list of entities, follow `prop` links (e.g., 'object') and return
    all linked entities whose `@id` includes `substring`.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Index entities by ID for fast lookup
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }
    matched = []
    for entity in entities:
        val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
        if val is None:
            continue

        vals = [val] if not isinstance(val, list) else val

        for v in vals:
            target_id = (
                v.id if hasattr(v, "id") else v.get("@id") if isinstance(v, dict) else v
            )

            if not isinstance(target_id, str):
                continue

            if substring in target_id:
                linked_entity = id_index.get(target_id)
                if linked_entity:
                    matched.append(linked_entity)

    return matched

def resolve_linked_entity(crate, entity, prop):
    """
    Follow a single-valued property (like 'instrument') from an entity,
    and return the linked entity (resolved from the crate).
    Returns None if the property is missing or not resolvable.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Build a lookup of @id to entity
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }

    val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
    if val is None:
        return None

    # Normalize to string ID
    if isinstance(val, list):
        raise ValueError(f"Expected only one linked entity in property '{prop}', but found a list.")
    
    target_id = (
        val.id if hasattr(val, "id") else val.get("@id") if isinstance(val, dict) else val
    )

    return id_index.get(target_id)

def convert_to_raw_url(github_url: str) -> str:
    """
    Converts a GitHub blob URL to a raw.githubusercontent URL.
    """
    match = re.match(r"https://github\.com/(.+)/blob/([a-f0-9]+)/(.+)", github_url)
    if not match:
        raise ValueError("Invalid GitHub blob URL format.")
    user_repo, commit_hash, path = match.groups()
    return f"https://raw.githubusercontent.com/{user_repo}/{commit_hash}/{path}"

def get_location_name(geometry):
    """
    Get a human-readable location name from a shapely geometry using reverse geocoding.
    Uses the centroid of the geometry for the lookup.
    """
    try:
        # Get the centroid of the geometry
        centroid = geometry.centroid
        lon, lat = centroid.x, centroid.y
        
        # Fix longitude coordinates that are in 0-360 range (convert to -180 to 180)
        if lon > 180:
            lon = lon - 360
        
        location = requests.get(
            url="https://nominatim.openstreetmap.org/reverse",
            params={
                'lat': lat,
                'lon': lon,
                'format': 'json',
                'zoom': 10,
                'addressdetails': 1
            },
            headers={'User-Agent': 'CoastSat-shoreline-publication'}
        ).json().get('display_name')
        
        return location
    except Exception as e:
        print(f"Error getting location name: {e}")
        return "Unknown location"

def get_appended_rows(
    old_url: str,
    new_url: str,
    filter_column: str = None,
    filter_value: str = None
) -> pd.DataFrame:
    raw_old_url = convert_to_raw_url(old_url)
    raw_new_url = convert_to_raw_url(new_url)
    
    old_df = pd.read_csv(raw_old_url)
    new_df = pd.read_csv(raw_new_url)
    
    appended_df = pd.concat([new_df, old_df]).drop_duplicates(keep=False)

    # If you're looking for a particular transect column (e.g., 'sar0003-0003'),
    # this assumes the filter_column is in the wide format as a column name.
    if filter_column and filter_column in appended_df.columns:
        # Only keep rows where that transect column is not NaN
        appended_df = appended_df[~appended_df[filter_column].isna()]

    return appended_df
```

```python exec
# Plotly visualisation functions
def plot_zone_transect_timeseries(site_time_series_df, zone_transects, zone_metrics):
    # Create a time series plot for the zone transects
    if not site_time_series_df.empty and zone_transects:
        site_time_series_df['dates'] = pd.to_datetime(site_time_series_df['dates'])
        fig = go.Figure()

        # Get valid transects
        transect_ids = [t['id'] for t in zone_transects]
        valid_transects = [(tid, site_time_series_df[['dates', tid]].dropna()) 
                           for tid in transect_ids if tid in site_time_series_df.columns]

        # Filter out transects with insufficient data
        valid_transects = [(tid, data) for tid, data in valid_transects 
                          if len(data) > 0 and len(data.columns) >= 2]

        if valid_transects:
            # Zone baseline: average of all values
            all_values = []
            for _, data in valid_transects:
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    all_values.extend(data.iloc[:, 1].tolist())
            
            if not all_values:
                print("No valid transect data available for zone analysis.")
                return
                
            zone_baseline = np.mean(all_values)

            # Efficient lookup with dicts for each transect
            transect_dicts = []
            for tid, data in valid_transects:
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    transect_dicts.append(
                        (tid, dict(zip(data['dates'], data.iloc[:, 1] - zone_baseline)))
                    )

            all_dates = sorted(set().union(*[data['dates'] for _, data in valid_transects]))

            # Compute zone average over time
            zone_data = []
            for date in all_dates:
                date_vals = [tdict[date] for _, tdict in transect_dicts if date in tdict]
                if date_vals:
                    zone_data.append((date, np.mean(date_vals)))

            zone_dates, zone_values = zip(*zone_data) if zone_data else ([], [])

            # Colors
            colors = px.colors.qualitative.Set1

            # Plot all individual transects (all visible)
            for i, (tid, data) in enumerate(valid_transects):
                if data.shape[1] >= 2:  # Ensure we have at least 2 columns
                    fig.add_trace(go.Scatter(
                        x=data['dates'],
                        y=data.iloc[:, 1] - zone_baseline,
                        mode='lines',
                        name=tid,
                        opacity=0.6,
                        visible=True,  # Show all
                        line=dict(color=colors[i % len(colors)], width=1),
                        hovertemplate='%{fullData.name}: %{y:.1f}m<extra></extra>'
                    ))

            # Smoothed zone average
            if len(zone_values) > 1:
                zone_array = np.array(zone_values)
                window = min(6, len(zone_array) // 4)
                if window >= 2:
                    kernel = np.ones(window) / window
                    smoothed = np.convolve(zone_array, kernel, mode='same')
                else:
                    smoothed = zone_array

                fig.add_trace(go.Scatter(
                    x=zone_dates,
                    y=smoothed,
                    mode='lines+markers',
                    name='Zone Average (Smoothed)',
                    line=dict(color='black', width=3),
                    marker=dict(size=4, color='black'),
                    hovertemplate='Zone Average: %{y:.1f}m<extra></extra>'
                ))

                # Linear trendline for zone average
                if len(zone_dates) > 2:
                    dates_numeric = pd.to_numeric(pd.Series(zone_dates))
                    slope, intercept, *_ = stats.linregress(dates_numeric, zone_values)
                    trend_line = slope * dates_numeric + intercept
                    annual_rate = slope * 365.25 * 24 * 3600 * 1000  # m/ms → m/yr

                    fig.add_trace(go.Scatter(
                        x=zone_dates,
                        y=trend_line,
                        mode='lines',
                        name=f'Zone Trend ({annual_rate:.1f} m/yr)',
                        line=dict(color='red', width=2, dash='dash'),
                        hovertemplate='Trend: %{y:.1f}m<extra></extra>'
                    ))

            # Add baseline reference
            fig.add_hline(
                y=0,
                line_dash="dot",
                line_color="gray",
                opacity=0.5,
                annotation_text="Zone Baseline",
                annotation_position="bottom right"
            )

            # Layout and styling
            fig.update_layout(
                title=f'Shoreline Position Time Series - {zone_metrics.zone_type} Zone<br><sub>Relative to zone mean baseline</sub>',
                xaxis_title='Date',
                yaxis_title='Cross-shore change [m]',
                hovermode='x unified',
                width=800,
                height=400,
                margin=dict(l=50, r=150, t=60, b=60),
                font=dict(size=10),
                legend=dict(
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02,
                    font=dict(size=9),
                    bordercolor="LightGray",
                    borderwidth=1,
                    itemsizing='trace'
                )
            )

            fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#eee')
            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#eee', hoverformat='.1f')

            # Set x-range to emphasize recent data
            if zone_dates and len(zone_dates) > 0:
                end_date = pd.to_datetime(zone_dates[-1])
                start_date = pd.to_datetime(zone_dates[0])
                data_span = (end_date - start_date).days / 365.25
                view_start = end_date - pd.DateOffset(years=20) if data_span > 20 else start_date - pd.DateOffset(years=1)
                view_end = end_date + pd.DateOffset(years=1)
                fig.update_xaxes(range=[view_start, view_end])

            fig.show()
        else:
            print("No time series data available or no transects in zone.")

def plot_zone_uncertainty_metrics(site_time_series_df, zone_transects, window_size=12):
    def compute_metrics_over_time(df, window_size=window_size):
        df = df.dropna()
        if len(df) < window_size:
            return []
        
        # Check if we have the required columns
        if len(df.columns) < 2:
            print(f"Warning: DataFrame has insufficient columns: {list(df.columns)}")
            return []

        df = df.sort_values('dates')
        df['ordinal'] = pd.to_datetime(df['dates']).map(pd.Timestamp.toordinal)
        metrics = []

        for i in range(len(df) - window_size + 1):
            window = df.iloc[i:i+window_size]
            X = window[['ordinal']]
            
            # Safely access the second column (transect data)
            if window.shape[1] < 2:
                continue
            y = window.iloc[:, 1]
            
            # Skip if y has no valid data
            if len(y) == 0 or y.isna().all():
                continue
                
            model = LinearRegression().fit(X, y)
            preds = model.predict(X)
            r2 = r2_score(y, preds)
            rmse = sqrt(mean_squared_error(y, preds))
            center_date = window['dates'].iloc[window_size // 2]
            metrics.append((center_date, r2, rmse))

        return metrics

    # 2. Generate metrics across all transects
    metrics_data = []
    transect_ids = [t['id'] for t in zone_transects]

    for tid in transect_ids:
        if tid not in site_time_series_df.columns:
            continue
        df = site_time_series_df[['dates', tid]].dropna()
        for date, r2, rmse in compute_metrics_over_time(df, window_size=window_size):
            metrics_data.append({
                'transect_id': tid,
                'date': date,
                'r2': r2,
                'rmse': rmse
            })

    metrics_df = pd.DataFrame(metrics_data)

    # 3. Clean and group by monthly bins
    metrics_df['date'] = pd.to_datetime(metrics_df['date'], errors='coerce')
    metrics_df = metrics_df.dropna(subset=['date'])
    metrics_df = metrics_df[(metrics_df['date'] > '1985-01-01') & (metrics_df['date'] < '2025-01-01')]
    metrics_df['date_bin'] = metrics_df['date'].dt.to_period('M').dt.to_timestamp()

    agg_iqr = metrics_df.groupby('date_bin').agg(
        r2_q1=('r2', lambda x: x.quantile(0.25)),
        r2_q3=('r2', lambda x: x.quantile(0.75)),
        r2_median=('r2', 'median'),
        rmse_q1=('rmse', lambda x: x.quantile(0.25)),
        rmse_q3=('rmse', lambda x: x.quantile(0.75)),
        rmse_median=('rmse', 'median')
    ).reset_index().rename(columns={'date_bin': 'date'})

    # 4. Compute RMSE trend line (linear regression)
    agg_iqr_clean = agg_iqr.dropna(subset=['rmse_median']).copy()
    agg_iqr_clean['ordinal'] = agg_iqr_clean['date'].map(pd.Timestamp.toordinal)

    X = agg_iqr_clean[['ordinal']]
    y = agg_iqr_clean['rmse_median']
    model = LinearRegression().fit(X, y)
    agg_iqr_clean['rmse_trend'] = model.predict(X)

    # 5. Create ghost lines for individual transects
    ghost_r2 = []
    ghost_rmse = []

    for tid in metrics_df['transect_id'].unique():
        df = metrics_df[metrics_df['transect_id'] == tid].sort_values('date')
        
        ghost_r2.append(go.Scatter(
            x=df['date'], y=df['r2'],
            mode='lines',
            line=dict(width=1, color='rgba(34,139,34,0.1)'),  # faint green
            showlegend=False, hoverinfo='skip'
        ))
        
        ghost_rmse.append(go.Scatter(
            x=df['date'], y=df['rmse'],
            mode='lines',
            line=dict(width=1, color='rgba(255,140,0,0.1)'),  # faint orange
            showlegend=False, hoverinfo='skip'
        ))

    # 6. Create final plot with subplots
    fig = make_subplots(
        rows=2, cols=1, shared_xaxes=True,
        subplot_titles=('Median R² Score with IQR Band', 'Median RMSE with IQR Band')
    )

    for trace in ghost_r2:
        fig.add_trace(trace, row=1, col=1)
    for trace in ghost_rmse:
        fig.add_trace(trace, row=2, col=1)

    # Median and IQR plots
    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_median'],
        mode='lines', name='R² Median',
        line=dict(color='seagreen')
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_q3'],
        mode='lines', line=dict(width=0), showlegend=False
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['r2_q1'],
        mode='lines', fill='tonexty',
        fillcolor='rgba(46,139,87,0.3)',
        line=dict(width=0), name='R² IQR'
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_median'],
        mode='lines', name='RMSE Median',
        line=dict(color='darkorange')
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_q3'],
        mode='lines', line=dict(width=0), showlegend=False
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr['date'], y=agg_iqr['rmse_q1'],
        mode='lines', fill='tonexty',
        fillcolor='rgba(255,165,0,0.3)',
        line=dict(width=0), name='RMSE IQR'
    ), row=2, col=1)

    fig.add_trace(go.Scatter(
        x=agg_iqr_clean['date'], y=agg_iqr_clean['rmse_trend'],
        mode='lines', name='RMSE Trend (Linear Fit)',
        line=dict(color='black', dash='dash')
    ), row=2, col=1)

    # Updated layout for responsiveness
    fig.update_layout(
        autosize=True,
        template='plotly_white',
        hovermode='x unified',
        margin=dict(l=40, r=40, t=80, b=40),
        legend=dict(
            orientation="h",
            yanchor="top",
            y=-0.25,
            xanchor="center",
            x=0.5,
            font=dict(size=10)
        )
    )

    fig.update_yaxes(title_text='R² Score', range=[0, 1], row=1, col=1)
    fig.update_yaxes(title_text='RMSE (m)', row=2, col=1)
    fig.update_xaxes(title_text='Date', row=2, col=1)

    # Show with responsive config
    fig.show(config={'responsive': True})

# Folium map creation function
# Create Maps which visualise transects - composable function for different use cases
def create_transect_map(transects_data, map_title="Transects Map", use_gdf=False):
    """
    Create a Folium map with trend-colored transects.
    
    Args:
        transects_data: Either a list of transect dicts (zone case) or GeoDataFrame (site case)
        map_title: Title to display on the map
        use_gdf: If True, treats transects_data as a GeoDataFrame; if False, as transect list
    
    Returns:
        Folium map object
    """
    
    def get_trend_color(trend, n_points=None):
        """Get color based on trend value, matching the index.html color scheme"""
        # Handle insufficient data points (gray)
        if n_points is not None and n_points < 10:
            return '#BABABA'
        
        # Handle null trends (gray)
        if trend is None or pd.isna(trend):
            return '#808080'
        
        # Apply the same color scale as index.html: RdYlBu with domain [-3, 3]
        # Clamp trend values to the domain range
        clamped_trend = max(-3, min(3, trend))
        
        # RdYlBu color scale interpolation (Red-Yellow-Blue)
        # Red for negative (erosion), Yellow for ~0 (stable), Blue for positive (accretion)
        if clamped_trend <= -2:
            return '#d73027'  # Dark red
        elif clamped_trend <= -1:
            return '#f46d43'  # Red-orange
        elif clamped_trend <= -0.5:
            return '#fdae61'  # Orange
        elif clamped_trend <= 0:
            return '#fee08b'  # Light orange/yellow
        elif clamped_trend <= 0.5:
            return '#e6f598'  # Light green/yellow
        elif clamped_trend <= 1:
            return '#abdda4'  # Light blue/green
        elif clamped_trend <= 2:
            return '#66c2a5'  # Medium blue/green
        else:
            return '#3288bd'  # Blue

    # Convert data to GeoDataFrame if needed
    if use_gdf:
        # transects_data is already a GeoDataFrame (site case)
        gdf = transects_data.copy()
    else:
        # transects_data is a list of transect dicts (zone case)
        features = []
        for t in transects_data:
            coords = t["geometry"]["coordinates"]
            line = LineString(coords)
            # Copy all properties from t["properties"], plus id and site_id at top level
            props = dict(t.get("properties", {}))
            props["id"] = t["id"]
            props["site_id"] = t.get("site_id")
            props["geometry"] = line
            features.append(props)
        gdf = gpd.GeoDataFrame(features, crs="EPSG:4326")

    # Calculate bounds and center
    bounds = gdf.total_bounds
    center = [(bounds[1] + bounds[3]) / 2, (bounds[0] + bounds[2]) / 2]

    # Create map
    m = folium.Map(location=center, zoom_start=15, tiles="OpenStreetMap")

    # Rounding configuration for tooltips
    rounding_map = {
        "orientation": ("°", 1),
        "along_dist": ("m", 1),
        "along_dist_norm": ("", 3),
        "beach_slope": ("", 2),
        "cil": ("", 3),
        "ciu": ("", 3),
        "trend": ("", 2),
        "r2_score": ("", 3),
        "mae": ("m", 1),
        "mse": ("m²", 1),
        "rmse": ("m", 1),
        "intercept": ("", 1)
    }

    # Build tooltip fields and aliases
    # Get available columns from the GeoDataFrame
    available_cols = [col for col in gdf.columns if col not in ["geometry", "site_id"]]
    
    tooltip_fields = []
    tooltip_aliases = []
    
    # Always include ID if available
    if "id" in available_cols:
        tooltip_fields.append("id")
        tooltip_aliases.append("Transect ID:")
    
    # Add other available fields
    for key in available_cols:
        if key != "id":  # Already handled above
            tooltip_fields.append(key)
            alias = key.replace("_", " ").capitalize()
            if key in rounding_map:
                unit, _ = rounding_map[key]
                if unit:
                    alias += f" ({unit})"
            tooltip_aliases.append(alias + ":")

    # Style function for each feature
    def style_function(feature):
        props = feature['properties']
        trend = props.get('trend')
        n_points = props.get('n_points_nonan')
        
        color = get_trend_color(trend, n_points)
        
        return {
            'color': color,
            'weight': 3,
            'opacity': 0.8
        }

    # Add GeoJSON layer with styling and tooltips
    folium.GeoJson(
        gdf.to_json(),
        name="Transects",
        style_function=style_function,
        tooltip=folium.GeoJsonTooltip(fields=tooltip_fields, aliases=tooltip_aliases, sticky=True, localize=True)
    ).add_to(m)

    # Fit map to bounds so all transects are visible
    m.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])
    
    # Add compact legend
    legend_html = '''
    <div style="position: fixed; 
                bottom: 20px; right: 20px; width: 120px; height: auto; 
                background-color: rgba(0, 0, 0, 0.85); border-radius: 4px; 
                color: white; padding: 8px; font-size: 10px; z-index: 9999; line-height: 1.2;">
        <div style="font-weight: bold; margin-bottom: 4px; font-size: 11px;">Trend (m/yr)</div>
        <div style="margin-bottom: 2px;"><i style="background: #3288bd; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>≥3 Accretion</div>
        <div style="margin-bottom: 2px;"><i style="background: #66c2a5; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>2</div>
        <div style="margin-bottom: 2px;"><i style="background: #abdda4; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>1</div>
        <div style="margin-bottom: 2px;"><i style="background: #e6f598; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>0 Stable</div>
        <div style="margin-bottom: 2px;"><i style="background: #fee08b; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>-1</div>
        <div style="margin-bottom: 2px;"><i style="background: #fdae61; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>-2</div>
        <div style="margin-bottom: 2px;"><i style="background: #d73027; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>≤-3 Erosion</div>
        <div style="margin-bottom: 0px;"><i style="background: #BABABA; width: 12px; height: 12px; float: left; margin-right: 5px; margin-top: 1px;"></i>Uncertain</div>
    </div>
    '''
    m.get_root().html.add_child(folium.Element(legend_html))
    
    # Add centered title
    title_html = f'<h2 style="position:absolute;z-index:100000;left:50%;top:10px;transform:translateX(-50%);font-size:1.2em;font-weight:400;color:#444;background:rgba(255,255,255,0.7);padding:4px 12px;border-radius:6px;box-shadow:0 1px 4px rgba(0,0,0,0.08);">{map_title}</h2>'
    m.get_root().html.add_child(folium.Element(title_html))

    return m
```

```python exec
# Get data for the site report - use cached data files
cached_shoreline_path = Path.cwd() / "cached_shoreline.geojson"
if cached_shoreline_path.exists():
    shoreline_gdf = gpd.read_file(cached_shoreline_path)
else:
    print("Error: No cached shoreline data found. Run publication_logic.py to cache data first.")
    raise FileNotFoundError("Cached shoreline data not found")

current_site_data = shoreline_gdf[shoreline_gdf['id'] == site_id] # Filter the shoreline data by the current site_id
```

```python exec
# Get location name in a separate block to avoid serialization issues
try:
    if len(current_site_data) > 0:
        geometry = current_site_data['geometry'].iloc[0]
        location_name = get_location_name(geometry)
    else:
        location_name = "No data found for this site"
except Exception as e:
    print(f"Warning: Could not get location name: {e}")
    location_name = "Unknown location"
```

```python exec
# Get data for the site report - use cached primary result data
cached_primary_result_path = Path.cwd() / "cached_primary_result.geojson"
if cached_primary_result_path.exists():
    primary_result_gdf = gpd.read_file(cached_primary_result_path)
else:
    print("Error: No cached primary result data found. Run publication_logic.py to cache data first.")
    raise FileNotFoundError("Cached primary result data not found")

site_primary_data = primary_result_gdf[primary_result_gdf['site_id'] == site_id]
```

# Site Report: `site_id`{python exec}
### Location: `location_name`{python exec}
#### Generated from CoastSat version committed on `coastsat_commit_date`{python exec}

```python exec
import sys
sys.path.insert(0, str(Path.cwd()))
from narrative_zoning import run_narrative_zoning

aoi_definitions =  {
    "No Data": {
        "priority": 1,
        "conditions": [
        {"field": "trend", "operator": "is_null", "value": None}
        ],
        "description_template": "No data zone where insufficient observations prevent trend analysis."
    },
    "Rapid Erosion": {
        "priority": 2,
        "conditions": [
        {"field": "trend", "operator": "<", "value": -0.8}
        ],
        "description_template": "Critical erosion hotspot with average retreat of {avg_trend_abs:.1f}m/year. This area requires immediate attention and monitoring."
    },
    "Moderate Erosion": {
        "priority": 3,
        "conditions": [
        {"field": "trend", "operator": "<", "value": -0.3}
        ],
        "description_template": "Erosion zone showing consistent retreat averaging {avg_trend_abs:.1f}m/year. Ongoing erosion processes are evident."
    },
    "Rapid Accretion": {
        "priority": 4,
        "conditions": [
        {"field": "trend", "operator": ">", "value": 0.8}
        ],
        "description_template": "Dynamic accretion zone with significant sand accumulation averaging {avg_trend:.1f}m/year. This area shows strong sediment deposition."
    },
    "Moderate Accretion": {
        "priority": 5,
        "conditions": [
        {"field": "trend", "operator": ">", "value": 0.3}
        ],
        "description_template": "Stable accretion zone with gradual beach building averaging {avg_trend:.1f}m/year. Positive sediment balance is maintained."
    },
    "High Uncertainty": {
        "priority": 6,
        "conditions": [
        {"field": "r2_score", "operator": "<", "value": 0.05, "allow_null": True},
        {"field": "rmse", "operator": ">", "value": 30, "allow_null": True}
        ],
        "logic": "OR",  # Either condition can trigger this zone
        "description_template": "Data-limited zone where shoreline trends are difficult to determine reliably. Additional monitoring may be needed."
    },
    "Steep Beach": {
        "priority": 7,
        "conditions": [
        {"field": "beach_slope", "operator": ">", "value": 0.08}
        ],
        "description_template": "High-energy beach zone characterized by steep beach profiles. This area may be vulnerable to storm impacts."
    },
    "Low Energy": {
        "priority": 8,
        "conditions": [
        {"field": "beach_slope", "operator": "<", "value": 0.04}
        ],
        "description_template": "Protected shoreline segment with gentle beach profiles indicating low wave energy conditions."
    },
    "Stable": {
        "priority": 9,
        "conditions": [],  # Default catch-all zone
        "description_template": "Stable shoreline segment showing minimal change over time. This area exhibits natural equilibrium."
    }
    }
```

```python exec
# Handle potential null/None values in the data
numeric_data = site_primary_data.select_dtypes(include="number")

# Check if we have any numeric data to analyze
if len(numeric_data.columns) == 0 or len(numeric_data) == 0:
    # Create a placeholder summary for sites with no numeric data
    summary = pd.DataFrame({
        'count': [0], 'mean': [None], 'min': [None], 'max': [None], 'std': [None]
    }, index=['placeholder'])
    has_numeric_data = False
else:
    # Drop columns that are all null/nan
    numeric_data_clean = numeric_data.dropna(axis=1, how='all')
    
    if len(numeric_data_clean.columns) == 0:
        # All numeric columns are null
        summary = pd.DataFrame({
            'count': [0], 'mean': [None], 'min': [None], 'max': [None], 'std': [None]
        }, index=['placeholder'])
        has_numeric_data = False
    else:
        summary = numeric_data_clean.agg(["count", "mean", "min", "max", "std"]).transpose()
        summary = summary.round(3)
        has_numeric_data = True

# Helper function to safely get summary values
def get_summary_value(metric, stat, default="N/A"):
    try:
        if not has_numeric_data or metric not in summary.index:
            return default
        value = summary.loc[metric, stat]
        if pd.isna(value) or value is None:
            return default
        return value
    except (KeyError, IndexError):
        return default
```

> [!info]+ Site Zone Configuration
> ```python exec echo
> min_zone_length = max(3, int(len(site_primary_data) / 10))  # Minimum of 3, or len(site_primary_data)//3
> 
> # Initialize default values
> aoi_zones = []
> all_transects = []
> 
> try:
>     # Get areas of interest (AOIs) for the site
>     aois = run_narrative_zoning(site_id, cached_primary_result_path, min_zone_length=min_zone_length, zone_definitions=aoi_definitions, sort_by_priority=True)
>     aoi_zones = aois.get('zones', [])
>     all_transects = aois.get('transects', [])
> except Exception as e:
>     print(f"Warning: Narrative zoning failed for site {site_id}: {e}")
>     # Keep the default empty values
> ```


<!--- The following conditional blocks pre-process the timeseries data for different site types --->
::: if "nzd" in site_id or "ber" in site_id

```python exec
# If the site is a New Zealand or Bermuda site, we access its tidally corrected time series data
time_series_data_by_site = query_by_link(interface_crate, "exampleOfWork", "#fp-transecttimeseriestidallycorrected-2")
site_time_series_candidates = [
    e for e in time_series_data_by_site
    if site_id in (e.get("@id") if isinstance(e, dict) else getattr(e, "id", ""))
]
site_time_series_entity = site_time_series_candidates[0] if site_time_series_candidates else None  # Get the first match
site_time_series_url = convert_to_raw_url(site_time_series_entity["@id"]) if site_time_series_entity else None
# Download and load the site time series CSV data
if site_time_series_url:
    try:
        site_time_series_df = pd.read_csv(site_time_series_url)
    except Exception as e:
        print(f"Error loading site time series data: {e}")
        site_time_series_df = pd.DataFrame()
else:
    print("No site time series URL found.")
    site_time_series_df = pd.DataFrame()
```

::: elif "sar" not in site_id

```python exec 
# If the site is from the Pacific Rim, we access its time series data
time_series_data_by_site = query_by_link(interface_crate, "exampleOfWork", "#fp-timeseriestidallycorrected-1")
site_time_series_candidates = [
    e for e in time_series_data_by_site
    if site_id in (e.get("@id") if isinstance(e, dict) else getattr(e, "id", ""))
]
site_time_series_entity = site_time_series_candidates[0] if site_time_series_candidates else None  # Get the first match

site_time_series_url = convert_to_raw_url(site_time_series_entity["@id"]) if site_time_series_entity else None
# Download and load the site time series CSV data
if site_time_series_url:
    try:
        site_time_series_df = pd.read_csv(site_time_series_url)
    except Exception as e:
        print(f"Error loading site time series data: {e}")
        site_time_series_df = pd.DataFrame()
else:
    print("No site time series URL found.")
    site_time_series_df = pd.DataFrame()
```

::: 
<!--- End of conditional block for data pre-processing --->

::: if aoi_zones is not []

:::: for zone in aoi_zones

```python exec
def extract_zone_metrics(zone):
    def get(key, default=0, round_to=None):
        val = zone.get(key, default)
        if val is None:
            return "N/A"
        return round(val, round_to) if round_to is not None else val

    metrics = {
        "zone_type": zone.get("zone_type", "Unknown"),
        "length_km": round(abs(zone.get("length_meters", 0)) / 1000, 2),
        "mean_trend": get("mean_trend", 0, 2),
        "avg_beach_slope": get("avg_beach_slope", 0, 4),
        "r2_score": get("avg_r2", 0, 4),
        "avg_rmse": get("avg_rmse", None, 2),
        "avg_mae": get("avg_mae", None, 2),
        "avg_cil": get("avg_cil", None, 4),
        "avg_ciu": get("avg_ciu", None, 4),
        "avg_orientation": get("avg_orientation", None, 1),
        "narrative_description": zone.get("narrative_description", "No description available."),
        "transect_count": zone.get("transect_count", 0),
        "transect_ids": zone.get("transect_ids", []),
        "start_transect_id": zone.get("start_transect_id", "N/A"),
        "end_transect_id": zone.get("end_transect_id", "N/A"),
        "zone_name": zone.get("zone_name", "Unnamed Zone"),
    }

    return SimpleNamespace(**metrics)

zone_metrics = extract_zone_metrics(zone)
```

```python exec 
def get_zone_transects(zone_metrics, all_transects):
    """Helper function to extract transects for a specific zone"""
    id_set = set(zone_metrics.transect_ids)
    results = []
    for tid, t in all_transects.items():
        if tid in id_set:
            t_copy = dict(t)
            t_copy["id"] = tid
            results.append(t_copy)
    return results

# Usage for zone-specific map (current case)
zone_transects = get_zone_transects(zone_metrics, all_transects) 
```

::::

::: else

No areas of interest (AOIs) were found for the site.

:::

```python exec
# Check if date limits are set, and if so 
# create another section for the date limits
from_date_was_set = "from_date" in locals()
to_date_was_set = "to_date" in locals()

def human_readable_date(date_str):
    try:
        # Try parsing as DD-MM-YYYY
        dt = datetime.strptime(date_str, "%d-%m-%Y")
    except Exception:
        try:
            # Try parsing as YYYY-MM-DD
            dt = datetime.strptime(date_str, "%Y-%m-%d")
        except Exception:
            return date_str  # Return as is if parsing fails
    return dt.strftime("%B %d, %Y")

if not from_date_was_set:
    from_date = "01-01-1984"
if not to_date_was_set:
    to_date = datetime.today().strftime("%d-%m-%Y")

date_limit = from_date_was_set or to_date_was_set
from_date_human = human_readable_date(from_date)
to_date_human = human_readable_date(to_date)

```

::: if date_limit and "sar" not in site_id

## Time Delimited Analysis
### From `from_date_human`{python exec} to `to_date_human`{python exec}

```python exec
# Save original (full dataset) statistics for comparison
original_num_transects = len(site_primary_data)
original_avg_trend = get_summary_value("trend", "mean", 0)
original_min_trend = get_summary_value("trend", "min", 0)  
original_max_trend = get_summary_value("trend", "max", 0)
original_avg_r2 = get_summary_value("r2_score", "mean", 0)
original_avg_rmse = get_summary_value("rmse", "mean", 0)

# Format original statistics for display
original_avg_trend_formatted = f"{original_avg_trend:.2f}" if original_avg_trend != "N/A" else "N/A"
original_min_trend_formatted = f"{original_min_trend:.2f}" if original_min_trend != "N/A" else "N/A"
original_max_trend_formatted = f"{original_max_trend:.2f}" if original_max_trend != "N/A" else "N/A"
original_avg_r2_formatted = f"{original_avg_r2:.2f}" if original_avg_r2 != "N/A" else "N/A"
original_avg_rmse_formatted = f"{original_avg_rmse:.2f}" if original_avg_rmse != "N/A" else "N/A"

# Filter data by date range
if not site_time_series_df.empty:
    # Set variables for original data
    original_data_rows = site_time_series_df.shape[0]
    original_data_cols = site_time_series_df.shape[1]
    
    # Convert dates to datetime
    site_time_series_df_filtered = site_time_series_df.copy()
    site_time_series_df_filtered['dates'] = pd.to_datetime(site_time_series_df_filtered['dates'], utc=True)
    
    # Parse date limits (make them timezone-aware to match the data)
    from_date_dt = pd.to_datetime(from_date, format="%d-%m-%Y").tz_localize('UTC')
    to_date_dt = pd.to_datetime(to_date, format="%d-%m-%Y").tz_localize('UTC')
    
    # Apply date filter
    date_mask = (site_time_series_df_filtered['dates'] >= from_date_dt) & (site_time_series_df_filtered['dates'] <= to_date_dt)
    site_time_series_df_filtered = site_time_series_df_filtered[date_mask]
    
    # Set variables for filtered data
    filtered_data_rows = site_time_series_df_filtered.shape[0]
    filtered_data_cols = site_time_series_df_filtered.shape[1]
    
    if len(site_time_series_df_filtered) > 0:
        filtered_date_start = site_time_series_df_filtered['dates'].min().strftime('%B %Y')
        filtered_date_end = site_time_series_df_filtered['dates'].max().strftime('%B %Y')
        
        # Get transect columns (exclude 'dates' and any other metadata columns)
        transect_columns = [col for col in site_time_series_df_filtered.columns if col not in ['dates', 'satname', 'Unnamed: 0']]
        num_transects_total = len(transect_columns)
        
        def calculate_time_bounded_trends(df, transect_cols):
            """
            Calculate linear regression trends for time-bounded data.
            Replicates the approach from CoastSat/linear_models.ipynb but simplified for one site.
            """
            # Convert dates to years since start (following CoastSat methodology)
            df_analysis = df.copy()
            df_analysis['years_since_start'] = (df_analysis['dates'] - df_analysis['dates'].min()).dt.days / 365.25
            
            trends = []
            for transect_id in transect_cols:
                # Get non-null data for this transect
                transect_data = df_analysis[['years_since_start', transect_id]].dropna()
                
                if len(transect_data) < 3:  # Need minimum 3 points for meaningful regression
                    continue
                    
                # Prepare data for sklearn
                X = transect_data['years_since_start'].values.reshape(-1, 1)
                y = transect_data[transect_id].values
                
                # Fit linear model
                from sklearn.linear_model import LinearRegression
                from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
                
                linear_model = LinearRegression().fit(X, y)
                y_pred = linear_model.predict(X)
                
                # Calculate metrics (following CoastSat structure)
                trend_data = {
                    'transect_id': transect_id,
                    'trend_time_bounded': linear_model.coef_[0],  # m/year
                    'intercept_time_bounded': linear_model.intercept_,
                    'n_points_time_bounded': len(df_analysis[transect_id]),  # Total points in period
                    'n_points_nonan_time_bounded': len(transect_data),  # Valid points used
                    'r2_score_time_bounded': r2_score(y, y_pred),
                    'mae_time_bounded': mean_absolute_error(y, y_pred),
                    'mse_time_bounded': mean_squared_error(y, y_pred),
                    'rmse_time_bounded': mean_squared_error(y, y_pred) ** 0.5,
                    'date_range_start': df_analysis['dates'].min().strftime('%Y-%m-%d'),
                    'date_range_end': df_analysis['dates'].max().strftime('%Y-%m-%d'),
                    'time_period_years': (df_analysis['dates'].max() - df_analysis['dates'].min()).days / 365.25
                }
                trends.append(trend_data)
                
            return pd.DataFrame(trends)
        
        # Calculate time-bounded trends
        time_bounded_trends = calculate_time_bounded_trends(site_time_series_df_filtered, transect_columns)
        
        if len(time_bounded_trends) > 0:
            # Set variables for use in prose (formatted for presentation)
            num_transects_analyzed = len(time_bounded_trends)
            avg_trend_time_bounded = f"{time_bounded_trends['trend_time_bounded'].mean():.2f}"
            min_trend_time_bounded = f"{time_bounded_trends['trend_time_bounded'].min():.2f}"
            max_trend_time_bounded = f"{time_bounded_trends['trend_time_bounded'].max():.2f}"
            avg_r2_time_bounded = f"{time_bounded_trends['r2_score_time_bounded'].mean():.2f}"
            total_observations_period = int(site_time_series_df_filtered[transect_columns].count().sum())
            avg_data_points = f"{time_bounded_trends['n_points_nonan_time_bounded'].mean():.1f}"
            time_period_years = f"{time_bounded_trends['time_period_years'].iloc[0]:.1f}"  # Same for all transects
            
            # Calculate comparison metrics
            if original_avg_trend_formatted != "N/A" and float(original_avg_trend_formatted) != 0:
                original_val = float(original_avg_trend_formatted)
                time_bounded_val = float(avg_trend_time_bounded)
                
                # Calculate absolute difference in trend magnitude
                absolute_diff = abs(time_bounded_val) - abs(original_val)
                
                # Use absolute change approach for more meaningful comparisons
                if abs(absolute_diff) >= 0.1:  # Significant change threshold
                    if absolute_diff > 0:
                        trend_change_formatted = f"{absolute_diff:.2f} m/year increase in magnitude"
                    else:
                        trend_change_formatted = f"{abs(absolute_diff):.2f} m/year decrease in magnitude"
                elif abs(absolute_diff) >= 0.05:  # Moderate change threshold
                    if absolute_diff > 0:
                        trend_change_formatted = f"{absolute_diff:.2f} m/year increase in magnitude"
                    else:
                        trend_change_formatted = f"{abs(absolute_diff):.2f} m/year decrease in magnitude"
                else:
                    # For very small changes, describe as minimal difference
                    trend_change_formatted = f"minimal change ({abs(absolute_diff):.3f} m/year difference in magnitude)"
            else:
                trend_change_formatted = "significant change"
            
            if original_avg_r2_formatted != "N/A":
                r2_comparison = "higher" if float(avg_r2_time_bounded) > float(original_avg_r2_formatted) else "lower"
                r2_reliability = "more" if float(avg_r2_time_bounded) > float(original_avg_r2_formatted) else "less"
                r2_description = f"This suggests the time-bounded period shows {r2_comparison} model reliability, indicating {r2_reliability} consistent shoreline behavior during the specified timeframe."
            else:
                r2_description = "The model quality metrics provide insights into the consistency of shoreline behavior during the specified timeframe."
            
            # Update the existing site_primary_data with time-bounded metrics
            # This follows the approach from CoastSat/linear_models.ipynb
            time_bounded_trends_indexed = time_bounded_trends.set_index('transect_id')
            
            # Create a mapping from transect_id to the DataFrame index for site_primary_data
            site_primary_data_copy = site_primary_data.copy()
            
            # Add time-bounded metrics to transects that have both geometry data and time series data
            for idx, row in site_primary_data_copy.iterrows():
                transect_id = row['id']
                if transect_id in time_bounded_trends_indexed.index:
                    # Add all the time-bounded metrics
                    for col in time_bounded_trends_indexed.columns:
                        if col != 'transect_id':  # Skip the transect_id column since it's now the index
                            site_primary_data_copy.at[idx, col] = time_bounded_trends_indexed.loc[transect_id, col]
            
            # Store the updated data for use in further analysis
            site_primary_data_with_time_bounded = site_primary_data_copy
            time_bounded_analysis_available = True
            
            # Create map with time-bounded transect data
            # Clean up the data to show only time-bounded metrics in tooltips
            site_primary_data_for_map = site_primary_data_with_time_bounded.copy()
            
            # Define mapping of time-bounded columns to their standard names for the map
            time_bounded_to_standard = {
                'trend_time_bounded': 'trend',
                'r2_score_time_bounded': 'r2_score', 
                'rmse_time_bounded': 'rmse',
                'mae_time_bounded': 'mae',
                'mse_time_bounded': 'mse',
                'n_points_nonan_time_bounded': 'n_points_nonan',
                'n_points_time_bounded': 'n_points',
                'intercept_time_bounded': 'intercept'
            }
            
            # Remove original columns that will be replaced with time-bounded versions
            columns_to_remove = list(time_bounded_to_standard.values())
            for col in columns_to_remove:
                if col in site_primary_data_for_map.columns:
                    site_primary_data_for_map = site_primary_data_for_map.drop(columns=[col])
            
            # Rename time-bounded columns to standard names for the map
            rename_dict = {}
            for time_bounded_col, standard_col in time_bounded_to_standard.items():
                if time_bounded_col in site_primary_data_for_map.columns:
                    rename_dict[time_bounded_col] = standard_col
            
            if rename_dict:
                site_primary_data_for_map = site_primary_data_for_map.rename(columns=rename_dict)
            
        else:
            # Set variables for failed analysis case
            num_transects_analyzed = 0
            time_bounded_analysis_available = False
            site_primary_data_with_time_bounded = site_primary_data.copy()
            
    else:
        # Set variables for no data in range case
        num_transects_analyzed = 0
        time_bounded_analysis_available = False
        site_primary_data_with_time_bounded = site_primary_data.copy()
        
else:
    # Set variables for no time series data case  
    num_transects_analyzed = 0
    time_bounded_analysis_available = False
    site_time_series_df_filtered = pd.DataFrame()
    transect_columns = []
    site_primary_data_with_time_bounded = site_primary_data.copy()
```

### Data Processing

The analysis began with the complete satellite-derived shoreline dataset containing `original_data_rows`{python exec} observations for `num_transects_total`{python exec} shore-normal transects. After applying the specified date range filter from `from_date_human`{python exec} to `to_date_human`{python exec}, the dataset was reduced to `filtered_data_rows`{python exec} observations for `num_transects_total`{python exec} shore-normal transects.

:::: template_describe [openai/gpt-5] @livepublication/coastsat/NEW_META_PROMPT {meta_data} ::::

::: if time_bounded_analysis_available

These observations span `time_period_years`{python exec} years, with an average of `avg_data_points`{python exec} data points per transect.
The time-bounded analysis reveals an average shoreline change rate of **`avg_trend_time_bounded`{python exec} meters per year** across all analysed transects. 
Individual transect trends ranged from `min_trend_time_bounded`{python exec} to `max_trend_time_bounded`{python exec} meters per year.
The analysis processed a total of `total_observations_period`{python exec} valid shoreline observations across all transects during the specified period.

The quality of the linear regression models is reflected in an average R² value of `avg_r2_time_bounded`{python exec}, representing the proportion of variance in shoreline position explained by calculated trend. 

```python exec
# Create map showing time-bounded transect trends
time_bounded_map = create_transect_map(
    site_primary_data_for_map, 
    map_title=f"Time-Bounded Analysis ({from_date_human} to {to_date_human}) - {site_id}", 
    use_gdf=True
)
time_bounded_map
```

### Comparison

The original analysis, based on the complete temporal record, included `original_num_transects`{python exec} transects with an average trend of **`original_avg_trend_formatted`{python exec} meters per year** (ranging from `original_min_trend_formatted`{python exec} to `original_max_trend_formatted`{python exec} meters per year).
The time-bounded analysis shows `num_transects_analyzed`{python exec} transects with an average trend of **`avg_trend_time_bounded`{python exec} meters per year** (ranging from `min_trend_time_bounded`{python exec} to `max_trend_time_bounded`{python exec} meters per year). This represents a **`trend_change_formatted`{python exec}** in average trend compared to the historical baseline.
Model quality also differs between approaches: the time-bounded analysis achieved an average R² of `avg_r2_time_bounded`{python exec} compared to `original_avg_r2_formatted`{python exec} for the full dataset. 
`r2_description`{python exec}

:::
:::